---
title       : Practical Machine Learning Course Project
subtitle    : 
author      : Marco DiPlacido
job         : 
logo        : 
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow   # 
url:
  lib: ../../librariesNew
  assets: ../../assets
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```


## About My Course Project

* Summary
* In Sample and Out of Sample error
* The code
* Conclusion

---

## Summary

It was imediatly clear to me that this was a classification problem.  We have a factor variable that we are trying to predict, in this case it is classe.  There were around 159 possible variables i could use as predictors, and i soon realized that a lot of them were missing values in our training data set.  To widdle down to my candidate predictors, I started by using a very simple approach where i would remove all columns as candidates that had 95% or more NA values.  I checked both the test data set and the quiz data set for columns that met this criteria.  At this point i was able to get the number of predictors in the 50's, i further cleaned the datasets by removing other columns like X, user_name, and other such columns that didn't make sense as predictors. 

I felt that if i could reduce the number of predictors down to the mimimal set that could capture the most information that that would be a good logical next step.  I used PCA from the caret package and that was able to create a new set of 26 predictors.   

Finally i needed to select a model to train my model on, the professor mentioned that Random Forests are very popular in classification problems - you often see their use in Kaggle competitions.  I decided to use the randomForeset package to build my model from these new set of features. 

---

## In Sample and Out of Sample error

The model was able to predict in-sample with 100% accuracy.  This was not of great intrest to me because i was more concerned about how this model would generalize on my held out 30% test set, seeing 100% accuracy there would be of concern to me, because it might be an indication of over-fitting.   However the results looked good and the confusionMatrix on that set showed that the model was not perfectly fitting that data, but mostly getting the classifications right with an accuracy of 97.5%.  At this point i was ready to try out my model on the Quiz set.

#### Accuracy in-sample

Confusion Matrix and Statistics
   
          | A  |  B  |  C |   D  |  E
      --- | --- | --- | --- | --- | ---
     A | 3906 |  0    |  0   |  0  | 0
     B |  0  | 2658  |  0  |  0  |   0
     C |   0  |  0   | 2396  |  0  |  0
     D |   0  |  0    | 0   | 2252 |   0
     E |   0  |  0    |  0   | 0   | 2525


###### Overall Statistics
                                     
                  Accuracy : 1          
                    95% CI : (0.9997, 1)
       No Information Rate : 0.2843     
       P-Value [Acc > NIR] : < 2.2e-16  
                                     
                     Kappa : 1          
    Mcnemar's Test P-Value : NA       

#### Accuracy out-sample

Confusion Matrix and Statistics
   
          | A  |  B  |  C |   D  |  E
      --- | --- | --- | --- | --- | ---
     A | 1659 |  7    |  1   |  6  | 1
     B |  21  | 1103  |  14  |  1  |   0
     C |   1  |  27   | 987  |  8  |  3
     D |   0  |  7    | 27   | 929 |   1
     E |   0  |  7    |  8   | 5   | 1062

###### Overall Statistics
                                          
               Accuracy : 0.9754          
                 95% CI : (0.9711, 0.9792)
    No Information Rate : 0.2856          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9688          
 Mcnemar's Test P-Value : 2.29e-06  

---

## The code

[The Code](./CourseProject.R)

---

## Conclusion

On the quiz set my model got 20/20 correct.  Looking back on my work i probably wouldn't have jumped right to PCA, it's possible that that was not necessary to produce a decent model, but I haven't gone back to verify that PCA really improved the accuracy of my model.  The work to clean the data and reduce my set of possible predictors down was where most of the time was spent in building this model, i didn't bother with centering and scaling my predictors (it's possible that PCA does that for me).

---
